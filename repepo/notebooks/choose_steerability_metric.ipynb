{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a Steering Efficiency Metric\n",
    "\n",
    "The goal of this notebook is to validate some intuitions and design choices about the steering metric. \n",
    "\n",
    "We are primarily focusing on the logit difference as our downstream metric, this has been used in prior work as a measure of indirect effect [Linear Representation Hypothesis, Sparse Feature Circuits]\n",
    "\n",
    "Caveats \n",
    "- Previous work in steering vectors uses the average key probability instead, we do a comparison w/ this metric \n",
    "- The steering metric doesn't differentiate examples by base propensity, we investigate whether steering is substantially different in these two situations\n",
    "- The steering metric doesn't differentiate positive vs negative steering performance, we investigate whether this matters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repepo.steering.sweeps.constants import (\n",
    "    ALL_ABSTRACT_CONCEPT_DATASETS,\n",
    "    ALL_TOKEN_CONCEPT_DATASETS, \n",
    "    ALL_LANGUAGES,\n",
    "    ALL_LLAMA_7B_LAYERS,\n",
    "    ALL_MULTIPLIERS\n",
    ")\n",
    "\n",
    "from repepo.steering.sweeps.configs import (\n",
    "    get_abstract_concept_config,\n",
    "    get_token_concept_config\n",
    ")\n",
    "\n",
    "from repepo.steering.run_sweep import (\n",
    "    run_sweep, \n",
    "    load_sweep_results\n",
    ")\n",
    "\n",
    "from repepo.steering.plots.utils import (\n",
    "    get_config_fields,\n",
    "    make_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sweep to run over. \n",
    "\n",
    "from itertools import product\n",
    "\n",
    "debug_setting = {\n",
    "    \"datasets\": [\"power-seeking-inclination\"],\n",
    "    \"layers\": [13],\n",
    "    \"multipliers\": [-1.0, 0.0, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "def iter_config(setting):\n",
    "    for dataset, layer, multiplier in product(\n",
    "        setting[\"datasets\"], \n",
    "        setting[\"layers\"], \n",
    "        setting[\"multipliers\"]\n",
    "    ):\n",
    "        yield get_abstract_concept_config(\n",
    "            dataset=dataset,\n",
    "            layer=layer,\n",
    "            multiplier=multiplier\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, run the sweep and load results. \n",
    "# If sweep was already run, set RUN = False.\n",
    "RUN = False\n",
    "\n",
    "configs = list(iter_config(debug_setting))\n",
    "if RUN:\n",
    "    run_sweep(configs, force_rerun_apply=True)\n",
    "\n",
    "results = load_sweep_results(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a DataFrame from the results.\n",
    "df = make_results_df(results)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in positive prob and negative prob for one example. \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df):\n",
    "    example = df.iloc[0]\n",
    "    df = df[df[\"test_positive_example.text\"] == example[\"test_positive_example.text\"]]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # Plot positive token logit, negative token logit.\n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"test_positive_token.logprob\", label=\"Positive logprob\", ax=ax)\n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"test_negative_token.logprob\", label=\"Negative logprob\", ax=ax)\n",
    "\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in positive token logit and negative token logit for one example. \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df):\n",
    "    example = df.iloc[0]\n",
    "    df = df[df[\"test_positive_example.text\"] == example[\"test_positive_example.text\"]]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # Plot positive token logit, negative token logit.\n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"test_positive_token.logit\", label=\"Positive logit\", ax=ax)\n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"test_negative_token.logit\", label=\"Negative logit\", ax=ax)\n",
    "    # Also plot the logit_mean\n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"test_positive_token.logit_mean\", label=\"Positive logit mean\", ax=ax)\n",
    "    # Also plot the logit_mean\n",
    "    # sns.lineplot(data=df, x=\"multiplier\", y=\"test_positive_token.logit_mean\", label=\"Logit mean\", ax=ax)\n",
    "    # sns.lineplot(data=df, x=\"multiplier\", y=\"test_negative_token.logit_mean\", label=\"Logit mean\", ax=ax)\n",
    "\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_steering_efficiency(\n",
    "    df: pd.DataFrame, \n",
    "    base_metric_name: str = \"logit_diff\"\n",
    "):\n",
    "    df = df.copy()\n",
    "    # Group by examples\n",
    "    fields_to_group_by = get_config_fields()\n",
    "    fields_to_group_by.remove(\"multiplier\")\n",
    "    fields_to_group_by += [\"test_positive_example.text\"]\n",
    "\n",
    "    grouped = df.groupby(fields_to_group_by)\n",
    "\n",
    "    def fit_linear_regression(df: pd.DataFrame):\n",
    "        # Fit a linear regression of the base metric on the multiplier\n",
    "        # Return the slope and error of the fit \n",
    "        x = df[\"multiplier\"].to_numpy()\n",
    "        y = df[base_metric_name].to_numpy()        \n",
    "        (slope, intercept), residuals, _, _, _ = np.polyfit(x, y, 1, full=True)\n",
    "        # Return a dataframe with the slope and residuals\n",
    "        return pd.DataFrame({\n",
    "            \"slope\": [slope],\n",
    "            \"residual\": [residuals.item()]\n",
    "        })\n",
    "\n",
    "    # Apply a linear-fit to each group using grouped.apply\n",
    "    slopes = grouped.apply(fit_linear_regression, include_groups = False)\n",
    "    df = df.merge(slopes, on=fields_to_group_by, how='left')\n",
    "    return df \n",
    "\n",
    "df = calculate_steering_efficiency(df)\n",
    "print(len(df))\n",
    "\n",
    "# Scatter plot of the slopes and residuals\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.scatterplot(data=df, x=\"slope\", y=\"residual\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )\n",
    "\n",
    "# Print the top 5 examples by slope. \n",
    "def print_top_k_by_slope(df, k: int = 5):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(\"slope\", ascending=False)\n",
    "    df = df[['test_positive_example.text', \n",
    "         'slope', \n",
    "         'residual', \n",
    "         'logit_diff', \n",
    "         'test_positive_token.logit', \n",
    "         'test_negative_token.logit'\n",
    "    ]]\n",
    "    df = df.drop_duplicates(subset=['test_positive_example.text'])\n",
    "    pretty_print(df.head(k))\n",
    "\n",
    "print_top_k_by_slope(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative logit diff here means that the model was initially going to give the wrong answer, but was able to give the right an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"slope\", y=\"residual\", hue=\"logit_diff\", palette=\"icefire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks: \n",
    "- Here, we want the steering efficiency to be high, while residual is low (i.e. bottom right corner is best). \n",
    "- We observe that the best steering occurs for examples where the logit difference was around 0 (i.e. the model was already uncertain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does high residual look like?  \n",
    "\n",
    "Intuitively this means the steering effect is not a line. We visualize logit diff vs multiplier for the examples with top 3 residual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df):   \n",
    "    # Select top 3 and bottom 3 examples by residual\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df['multiplier'] == 0]\n",
    "    temp_df = temp_df.sort_values(\"residual\", ascending=False)\n",
    "    # Assert no duplicates\n",
    "    assert temp_df['test_positive_example.text'].is_unique\n",
    "    # temp_df = temp_df.drop_duplicates(subset=['test_positive_example.text'])\n",
    "    top_3 = temp_df.head(3)['test_positive_example.text']\n",
    "    bottom_3 = temp_df.tail(3)['test_positive_example.text']\n",
    "    # select middle 3 examples\n",
    "    # middle_3 = temp_df.iloc[(len(temp_df) // 2 - 1):(len(temp_df) // 2 + 2)]['test_positive_example.text']\n",
    "    combined = pd.concat([top_3, bottom_3])\n",
    "\n",
    "    # Create 'group' category for whether the example is top, middle or bottom\n",
    "    df['group'] = 'other'\n",
    "    df.loc[df['test_positive_example.text'].isin(top_3), 'group'] = 'top'\n",
    "    # df.loc[df['test_positive_example.text'].isin(middle_3), 'group'] = 'middle'\n",
    "    df.loc[df['test_positive_example.text'].isin(bottom_3), 'group'] = 'bottom'\n",
    "\n",
    "    # Filter df by the selected examples\n",
    "    df = df[df['test_positive_example.text'].isin(combined)]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # Plot logit diff. \n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"logit_diff\", hue=\"group\", ax=ax)\n",
    "\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks: \n",
    "- Even when the residual is high, it seems the effect is still monotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Investigating the effect of base propensity\n",
    "\n",
    "The steering metric doesn't differentiate examples by base propensity. \n",
    "As a result, we investigate whether steering is substantially different in these two situations\n",
    "\n",
    "First, let's plot the base propensity of all examples in the dataset. Here, we use logit difference as a measure of propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of logit diff\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.histplot(data=df[df['multiplier'] == 0], x=\"logit_diff\", bins=20, ax=ax)\n",
    "# Add vertical line at 0\n",
    "ax.axvline(0, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick the top 3, middle 3, and bottom 3 examples and plot the logit diff vs multiplier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in positive token logit and negative token logit for one example. \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df):   \n",
    "    \"\"\" Plot the propensity vs multiplier for the top 3, middle 3, and bottom 3 examples by base propensity \"\"\"\n",
    "    # Select top 3 and bottom 3 examples by logit diff at zero multiplier\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df['multiplier'] == 0]\n",
    "    temp_df = temp_df.sort_values(\"logit_diff\", ascending=False)\n",
    "    # Assert no duplicates\n",
    "    assert temp_df['test_positive_example.text'].is_unique\n",
    "    # temp_df = temp_df.drop_duplicates(subset=['test_positive_example.text'])\n",
    "    top_3 = temp_df.head(3)['test_positive_example.text']\n",
    "    bottom_3 = temp_df.tail(3)['test_positive_example.text']\n",
    "    # select middle 3 examples\n",
    "    middle_3 = temp_df.iloc[(len(temp_df) // 2 - 1):(len(temp_df) // 2 + 2)]['test_positive_example.text']\n",
    "    combined = pd.concat([top_3, middle_3, bottom_3])\n",
    "\n",
    "    # Create 'group' category for whether the example is top, middle or bottom\n",
    "    df['group'] = 'other'\n",
    "    df.loc[df['test_positive_example.text'].isin(top_3), 'group'] = 'top'\n",
    "    df.loc[df['test_positive_example.text'].isin(middle_3), 'group'] = 'middle'\n",
    "    df.loc[df['test_positive_example.text'].isin(bottom_3), 'group'] = 'bottom'\n",
    "\n",
    "    # Filter df by the selected examples\n",
    "    df = df[df['test_positive_example.text'].isin(combined)]\n",
    "\n",
    "    # Print the average slope within group\n",
    "    print(df.groupby('group')['slope'].mean())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    # Plot logit diff. \n",
    "    sns.lineplot(data=df, x=\"multiplier\", y=\"logit_diff\", hue=\"group\", ax=ax)\n",
    "\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks: \n",
    "- Steering seems to \"work best\" when the base propensity is close to 0, reflecting that the model was uncertain. \n",
    "- Steering does not seem capable of influencing models' behaviour at very high or very low base propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a better way to visualize this, which is to do a scatter plot of (propensity) vs (base propensity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import seaborn.algorithms\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "\n",
    "def regplot_lowess_ci(data, x, y, ci_level, n_boot, ax, **kwargs):\n",
    "    x_ = data[x].to_numpy()\n",
    "    y_ = data[y].to_numpy()\n",
    "    x_grid = np.linspace(start=x_.min(), stop=x_.max(), num=1000)\n",
    "\n",
    "    def reg_func(_x, _y):\n",
    "        return lowess(exog=_x, endog=_y, xvals=x_grid)\n",
    "\n",
    "    beta_boots = seaborn.algorithms.bootstrap(\n",
    "        x_, y_,\n",
    "        func=reg_func,\n",
    "        n_boot=n_boot,\n",
    "    )\n",
    "    err_bands = sns.utils.ci(beta_boots, ci_level, axis=0)\n",
    "    y_plt = reg_func(x_, y_)\n",
    "\n",
    "    sns.lineplot(x=x_grid, y=y_plt, ax = ax, **kwargs)\n",
    "    sns.scatterplot(x=x_, y=y_, ax = ax, **kwargs)\n",
    "    ax.fill_between(x_grid, *err_bands, alpha=.15, **kwargs)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in positive token logit and negative token logit for one example. \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df):   \n",
    "    # Get \"base propensity\", which is the logit_diff at multiplier = 0\n",
    "    base_propensity = df[df['multiplier'] == 0]\n",
    "    base_propensity = base_propensity[['test_positive_example.text', 'logit_diff']]\n",
    "    # Rename logit diff to base logit diff\n",
    "    base_propensity = base_propensity.rename(columns={'logit_diff': 'base_logit_diff'})\n",
    "    assert base_propensity['test_positive_example.text'].is_unique\n",
    "    # Merge the base propensity into df\n",
    "    df = df.merge(base_propensity, on='test_positive_example.text', how='left')\n",
    "\n",
    "    # Scatter plot of propensity vs base propensity with hue as multiplier\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    palette = sns.diverging_palette(250, 30, l=65, center=\"dark\", as_cmap=True)\n",
    "    sns.scatterplot(data=df, y=\"logit_diff\", x=\"base_logit_diff\", hue=\"multiplier\", ax=ax, palette=palette)\n",
    "\n",
    "    # Plot slope vs base propensity. \n",
    "    # We use a nonparametric fit with lowess smoother to visualize the nonlinear relationship\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    regplot_lowess_ci(data=df, x=\"base_logit_diff\", y=\"slope\", ci_level=99, n_boot=100, ax=ax)\n",
    "\n",
    "    # # Plot nonparametric fit w/ lowess smoother. \n",
    "    # sns.regplot(data=df, x=\"base_logit_diff\", y=\"slope\", ax=ax,\n",
    "    #        lowess=True, line_kws={\"color\": \"C1\"})\n",
    "    \n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks. \n",
    "- Steering efficiency seems to be significantly lower when base propensity is high\n",
    "- One potential conclusion from this: steering cannot adjust a model's behaviour if it is already highly confident. \n",
    "\n",
    "Alternative hypotheses which need to be disambiguated to make this claim: \n",
    "- The steering vector extracted from all samples is simply OoD w.r.t the examples with high base propensity. To investigate these, we need to figure out (i) is the steering vector for high base propensity examples significantly different from that for the general example? (ii) if so, does steering with the high-propensity vector work on the high-propensity examples? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We set out to answer 3 questions: \n",
    "- Previous work in steering vectors uses the average key probability instead, we do a comparison w/ this metric \n",
    "- The steering metric doesn't differentiate examples by base propensity, we investigate whether steering is substantially different in these two situations\n",
    "- The steering metric doesn't differentiate positive vs negative steering performance, we investigate whether this matters\n",
    "\n",
    "\n",
    "We conclude:\n",
    "- TODO: compare w. average key probability (maybe not so important?)\n",
    "- Base propensity seems to be an important factor deciding the steerability of an example\n",
    "- Circumstantial evidence (Fig 1.1) that steerability is similar in both directions\n",
    "\n",
    "Caveats of this study:\n",
    "- Only did it for one dataset; need to see if trends hold for larger data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
