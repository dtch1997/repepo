{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repepo.experiments.steering.utils.helpers import (\n",
    "    SteeringConfig,\n",
    "    SteeringResult,\n",
    "    load_results\n",
    ")\n",
    "from repepo.experiments.steering.utils.configs import (\n",
    "    list_configs\n",
    ")\n",
    "\n",
    "configs = list_configs(\n",
    "    \"dev\",\n",
    "    train_split_name = \"train-dev\",\n",
    "    test_split_name = \"val-dev\",\n",
    ")\n",
    "\n",
    "all_results = []\n",
    "for config in configs:\n",
    "    results = load_results(config)\n",
    "    print(len(results))\n",
    "    all_results.append((config, results))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Plot Steering Performance\n",
    "\n",
    "Steering performance is measured in terms of the mean logit difference between (positive answer token) and (negative answer token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from repepo.experiments.steering.utils.helpers import SteeringConfig, SteeringResult\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_in_distribution_data_for_layer(\n",
    "    ax,\n",
    "    config: SteeringConfig,\n",
    "    results: list[SteeringResult]\n",
    "):\n",
    "    \n",
    "    # Create a colormap\n",
    "    colormap = plt.cm.get_cmap('tab10', len(config.layers))\n",
    "    \n",
    "    for i, layer in enumerate(config.layers):\n",
    "        layer_results = [x for x in results if x.layer_id == layer]\n",
    "        layer_results.sort(key=lambda x: x.multiplier)\n",
    "        \n",
    "        # Get the color for the current layer\n",
    "        color = colormap(layer)\n",
    "        \n",
    "        ax.plot(\n",
    "            [x.multiplier for x in layer_results],\n",
    "            [x.logit_diff for x in layer_results],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"dashed\",\n",
    "            markersize=5,\n",
    "            linewidth=2.5,\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "    ax.set_title(_get_title(config))\n",
    "    ax.set_xlabel(\"Multiplier\")\n",
    "    ax.set_ylabel(\"Mean logit difference\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    norm = mcolors.Normalize(vmin=min(config.layers), vmax=max(config.layers))\n",
    "    cbar = plt.colorbar(\n",
    "        plt.cm.ScalarMappable(\n",
    "            norm = norm, \n",
    "            cmap=colormap), \n",
    "        ax=ax\n",
    "    )\n",
    "    cbar.set_label(\"Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = plt.subplots(6, 1, figsize=(10, 20))\n",
    "\n",
    "for i, (config, results) in enumerate(all_results):\n",
    "    ax = axs[i]\n",
    "    plot_in_distribution_data_for_layer(ax, config, results)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compute Steering Efficiency\n",
    "\n",
    "Steering efficiency is computed as the slope of the linear regression line\n",
    "- where X is the multiplier\n",
    "- and Y is the mean-logit difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NewType\n",
    "\n",
    "multiplier_lower_bound = -1.0\n",
    "multiplier_upper_bound = 1.0\n",
    "DatasetName = NewType(\"DatasetName\", str)\n",
    "\n",
    "layers = tuple(configs[0].layers)\n",
    "\n",
    "all_slopes: dict[DatasetName, np.ndarray] = {}\n",
    "for config in configs:\n",
    "    result = load_results(config)\n",
    "    _layers = tuple(config.layers)\n",
    "    assert _layers == layers, \"Layers must be the same for all configs\"\n",
    "\n",
    "    slopes = []\n",
    "    for layer in layers:\n",
    "        # Calculate slope of best-fit line within range (-1, 1)\n",
    "        layer_results = [x for x in result if x.layer_id == layer]\n",
    "        layer_results.sort(key=lambda x: x.multiplier)\n",
    "        layer_results = [x for x in layer_results if multiplier_lower_bound <= x.multiplier <= multiplier_upper_bound]\n",
    "        x = [x.multiplier for x in layer_results]\n",
    "        y = [x.logit_diff for x in layer_results]\n",
    "        m, b = np.polyfit(x, y, 1)\n",
    "        slopes.append(m)\n",
    "    slopes  = np.array(slopes)\n",
    "    all_slopes[config.train_dataset_name] = slopes\n",
    "\n",
    "# Plot slopes by layer \n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for dataset_name, slopes in all_slopes.items():\n",
    "    axs.plot(layers, slopes, label=dataset_name)\n",
    "axs.legend()\n",
    "axs.set_title(\"Layer-wise steering efficiency in the range {} to {}\".format(multiplier_lower_bound, multiplier_upper_bound))\n",
    "axs.set_xlabel(\"Layer index\")\n",
    "axs.set_ylabel(\"Slope of best-fit line\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlate steering performance with metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pairwise cosine similarity gives us a notion of \"variance in direction\" \n",
    "- Variance of norms gives us a notion of \"variance in magnitude\"\n",
    "- Mean (scaled) euclidean distance to the mean-diff-vec is a notion of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repepo.experiments.steering.utils.metrics import list_metrics\n",
    "from repepo.experiments.steering.utils.notebook_utils import (\n",
    "    get_all_metrics, \n",
    "    get_all_concept_vectors, \n",
    "    plot_all_metrics, \n",
    ")\n",
    "\n",
    "metric_names = list_metrics()\n",
    "print(metric_names)\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    plot_all_metrics(configs, metric_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize correlations between metrics, steering efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of (dataset) x 10 matrices for metrics\n",
    "import numpy as np\n",
    "matrices: dict[str, np.ndarray] = {}\n",
    "\n",
    "slopes_mat = np.zeros(shape = (len(all_slopes), len(layers)))\n",
    "for i, (dataset_name, slopes) in enumerate(all_slopes.items()):\n",
    "    slopes_mat[i] = slopes\n",
    "matrices[\"steering_efficiency\"] = slopes_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name in metric_names:\n",
    "    metrics = get_all_metrics(configs, metric_name)\n",
    "    # assert len(metrics) == 6\n",
    "    # assert len(layers) == 10\n",
    "    metrics_mat = np.zeros(shape = (len(metrics), len(layers)))\n",
    "    for i, (dataset_name, layerwise_metrics) in enumerate(metrics.items()):\n",
    "        metric_values = [layerwise_metrics[k] for k in layers]\n",
    "        metrics_mat[i] = np.array(metric_values)\n",
    "    \n",
    "    matrices[metric_name] = metrics_mat\n",
    "\n",
    "# Invert the cosine similarity\n",
    "# matrices[\"cosine\"] = - matrices[\"cosine\"]\n",
    "# matrices[\"dot_product\"] = matrices[\"dot_product\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened_matrices = {k: v.flatten() for k, v in matrices.items()}\n",
    "_to_keep = (\n",
    "    \"cosine\",\n",
    "    \"euclidean_scaled_mean_norm\",\n",
    "    \"steering_efficiency\"\n",
    ")\n",
    "flattened_matrices = {k: flattened_matrices[k] for k in _to_keep}\n",
    "\n",
    "# Invert the variances to make them into similarity metrics\n",
    "flattened_matrices[\"euclidean_scaled_mean_norm\"] = -flattened_matrices[\"euclidean_scaled_mean_norm\"]\n",
    "# flattened_matrices[\"unscaled_var\"] = -flattened_matrices[\"unscaled_var\"]\n",
    "\n",
    "\n",
    "# Compute spearman correlation between each pair of metrics\n",
    "from scipy.stats import spearmanr\n",
    "correlation_matrix = np.zeros(shape = (len(flattened_matrices), len(flattened_matrices)))\n",
    "for i, (name1, mat1) in enumerate(flattened_matrices.items()):\n",
    "    for j, (name2, mat2) in enumerate(flattened_matrices.items()):\n",
    "        rho, p = spearmanr(mat1, mat2)\n",
    "        correlation_matrix[i, j] = rho\n",
    "\n",
    "# Plot correlation matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "cax = ax.matshow(correlation_matrix, cmap=\"coolwarm\", clim=(-1, 1))\n",
    "# Axis labels using _to_keep \n",
    "ax.set_xticks(range(len(_to_keep)))\n",
    "ax.set_yticks(range(len(_to_keep)))\n",
    "ax.set_xticklabels(_to_keep)\n",
    "ax.set_yticklabels(_to_keep)\n",
    "\n",
    "# Add values \n",
    "for i in range(len(_to_keep)):\n",
    "    for j in range(len(_to_keep)):\n",
    "        ax.text(i, j, f\"{correlation_matrix[j, i]:.3f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "\n",
    "print(correlation_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
